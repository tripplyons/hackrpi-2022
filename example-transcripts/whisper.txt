So while everyone was losing their minds into strain colab instances using stable diffusion, open AI dropped a bombshell. They released Whisper. But what exactly is Whisper? Well, it's an ASR, or automatic speech recognition model for short. But Nick, haven't we gone and solved speech recognition already? You are spot on, but Whisper is unique and we'll touch on some of the uniqueness in a second. But for now, what you really need to know is that it is a single model and it is amazing. And it's amazing because it's able to translate and transcribe multiple languages from audio based speech ridiculously easily. I'm talking three lines of code. So let me give you the machine learning for one one. Who, what, when, where and how the model was released by the open AI team on the 21st of September, 2022. It's unique in that it approaches human level robustness for transcription and translation using just a single model. This means that it can do quite a fair bit using a single trained machine learning model. But how exactly does it do this? It does it using a transformer encoder decoder architecture. Plus there's a couple of key nuances, namely the fact that it passes through special tokens in order to drive how the model actually works, whether or not to transcribe me speaking English to English text or whether or not to convert my speech into a completely different language. It automatically picks up the default task, but we can drive the outcome using a couple of keyword parameters in Python. Let's go jump in and some code. All right, let's slang some code. So if we want to go on ahead and get this installed, we can type in pip install and then pass through git plus and then the link to the GitHub repository, which is HTTPS column four slash forward slash GitHub.com. Or slash open AI for slash whisper dot get. And this is going to go on ahead and install all the dependencies that you're going to need to test out wispy yourself. But keep in mind this is not the GPU accelerated one. If you want GPU acceleration, you're gonna have to go on ahead and install the cruder equivalent of PyTorch. Okay, so that is it. Now in sold we can then import whisper. And then I've got a audio file over here. I don't have my headphones plugged in, so I have no idea what I went and said in this, but let's go and test it out. And let's play that. This is the test recording to see if we can leverage the whisper transcription. Yeah, I went and recorded this randomly during like some random meeting that I was on. Okay, so we'll be in part of Whisper then what we're going to go ahead and do is choose the model that we actually want to go and load. So I'll include a screenshot of all of the different available models up here. Now, what we actually want to do is import the base model to begin with. So we're going to type in model and then we're going to set that equal to whisper. load underscore model and we'll just load up base for now, which I think is kind of smallish. And then we'll go and see or stay tuned, we'll go and use a slightly larger model, we can then go and transcribe using out, we'll create a variable called out and set it equal to model dot transcribed. There are a couple of other different methods that you can use as well, not necessarily for transcription, but there's other functionality. So I'm then going to copy the part to my audio file. So this is an mp3 file. So you can see it's called quick clip dot mp3. Then we just pass clip.mp3, then we just pass through the file path to the transcribe method. And if I don't go and pass any other keyword parameters, this is just going to go and transcribe that audio clip. So you can see it is ridiculously easy to go and use. So this is a test recording to save. So you can see that we didn't say save. We can leverage the whisper transcription. I can't even speak it probably. So this is the test recording to ideally it should have said to see if we can leverage the whisper transcription. Now, right, so we can obviously go and test fix this up, but I'll show you how to do that in a second. But one of the other amazing things about whispers that you can obviously go and translate at the same time. So if I went and passed through language and set that equal to FR, I'll include a screenshot somewhere up here or a little bit of an awesome animation that shows you the other languages that are available. If I go and pass through language equals French or FR, this is actually going to not only transcribe the audio of my speech. it's also going to translate it to French. This is all in a single model guys, how awesome is that? Let's wait and see. And boom, there you go. Alright, let's test out my French. You know what, let's just stick to English or Australian. All right, so that's a, you can see that it's obviously gone and transcribed and translated it to French, right? Even though I have a French background, God, I'm a shocker. Okay, so let's just remove this French for now, or the French translation. And let's go and see if we can produce just a better baseline transcription. We can do that by using the medium model. And so obviously the larger model, I'm gonna use medium because it's a little bit smaller. So if I go and choose up here, it will pass through a slightly different parameter, up here it will pass through a slightly different parameter into our load model method. This is going to load up medium and now if we go and transcribe, ideally we should get slightly or significantly better results from whisper. Take a look at that. This is the test recording to see if we can leverage the whisper transcription. That is exactly what we said. Let's just play that audio file again This is the test recording to see if we can leverage the whisper transcription It is absolutely amazing that it's been able to go on ahead and do that literally guys one two three lines of code And you're able to get this up and running I personally think it's absolutely amazing and again We can go and pass through language equals French F.R. One is going transcribe it. How do we get Hindi? Alright, so this is French. I've worked out how to get Hindi. Cela test de la recording pour vois. C ompa vlav√© la transcription de wispel. Alright, but my personal favorite because I know there's a heap of Hindi speakers out there. We can type in hi and this should transcribe English to Hindi. Don't expect me But let's see this up and running. Drum roll please. Inside the drum roll audio editing neck now. Take a look at that. If any Hindi speakers watching this, let me know what that transcription is like. But you can see that it's obviously gone and converted English audio into Hindi. Now you could also pass through different levels or different languages of speech, and it's also able to transcribe that. Speech and it's also able to transcribe that I only speak English So I'm a little bit uncultured in that way, but I'd love to hear your results if you go and try this out Do let me know so that's whisper in a nutshell So remember it's just pipping sort of go and get it installed import whisper With the download model and you can pass through a number of different models in order to choose different grades of transcription and translation And then model dot transcribe and you can pass through the language parameter if you want to translate as well Let me know how you go. I'll catch in the next one. Happy one, hubbeats. So what do we think? Let me know in the comments below. I personally think it's quite an amazing fact that OpenAI has made this completely open and available and I truly like the fact that there's a number of different models. So if you want a model that's highly accurate and it's going to be bang on when it comes to performing a transcriptions, you can use one of those bigger models. But if you're running on like an edge device, a Raspberry Pi or a Jetson Nano, you can go down to that baby tiny model, which is going to give you some level of transcription, but you'll obviously have that compromise. Truly appreciate the fact that they've put some thought into actually building out that model. I think this is truly going to make it easier for people that are building user interfaces to add the voice interfaces into their applications, which means that we are improving accessibility and just making it that little bit easier for everyone to share in this amazing technology. Let me know where you think you'll be using it. I'll catch you in the next one. Hopefully we'll do some more ML shorts. Let me know if you like this format. Catch you later guys. Peace!
