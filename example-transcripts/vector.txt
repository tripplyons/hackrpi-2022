So while everyone was losing their minds into strain colab instances using stable diffusion, open AI dropped a bombshell. They released Whisper. But what exactly is Whisper? Well, it's an ASR, or automatic speech recognition model for short. But Nick, haven't we gone and solved speech recognition already? You are spot on, but Whisper is unique and we'll touch on some of the uniqueness in a second. But for now, what you really need to know is that it is a single model and it is amazing. And it's amazing because it's able to translate and transcribe multiple languages from audio based speech ridiculously easily. I'm talking three lines of code. So let me give you the machine learning for one one. Who, what, when, where and how the model was released by the open AI team on the 21st of September, 2022. It's unique in that it approaches human level robustness for transcription and translation using just a single model. This means that it can do quite a fair bit using a single trained machine learning model. But how exactly does it do this? It does it using a transformer encoder decoder architecture. Plus there's a couple of key nuances, namely the fact that it passes through special tokens in order to drive how the model actually works, whether or not to transcribe me speaking English to English text or whether or not to convert my speech into a completely different language. It automatically picks up the default task, but we can drive the outcome using a couple of keyword parameters in Python. Let's go jump in and some code. All right, let's slang some code. So if we want to go on ahead and get this installed, we can type in pip install and then pass through git plus and then the link to the GitHub repository, which is HTTPS column four slash forward slash GitHub.com. Or slash open AI for slash whisper dot get. And this is going to go on ahead and install all the dependencies that you're going to need to test out wispy yourself. But keep in mind this is not the GPU accelerated one.